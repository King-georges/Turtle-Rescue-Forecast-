{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "418b2df5",
   "metadata": {},
   "source": [
    "# This project involves developing a machine learning model to predict the number of turtles that will be rescued at each rescue site as part of the Local Ocean Conservation's by-catch release program. The following are the starter codes for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567835eb",
   "metadata": {},
   "source": [
    "# Step 1: Importing the necessary libraries and data\n",
    "In this step, we import the necessary libraries for the project, including Pandas for data manipulation, NumPy for numerical computations, scikit-learn for machine learning, and Matplotlib for data visualization. We also import the rescue data in CSV format into a Pandas dataframe for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d354b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_squared_error\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Importing the data\n",
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ebaf9f",
   "metadata": {},
   "source": [
    "# Step 2: Exploratory Data Analysis\n",
    "In this step, we explore the data to gain insights into its features and structure. We use the head(), describe(), and info() methods to display the first few rows, statistical summary, and information about the dataframe, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ed139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Rescue_ID Date_TimeCaught     Researcher    CaptureSite ForagingGround  \\\n",
      "0  2000_RE_0060      2000-12-22  Researcher_25  CaptureSite_0          Ocean   \n",
      "1  2001_RE_0187      2001-10-28   Researcher_6  CaptureSite_0          Ocean   \n",
      "2  2001_RE_0197      2001-11-01   Researcher_6  CaptureSite_0          Ocean   \n",
      "3  2002_RE_0031      2002-03-11  Researcher_32  CaptureSite_0          Ocean   \n",
      "4  2002_RE_0118      2002-08-08  Researcher_25  CaptureSite_0          Ocean   \n",
      "\n",
      "  CaptureMethod       Fisher                        LandingSite    Species  \\\n",
      "0           Net  Fisher_1072  LandingSite_CaptureSiteCategory_2  Species_6   \n",
      "1           Net   Fisher_520  LandingSite_CaptureSiteCategory_2  Species_6   \n",
      "2           Net  Fisher_1669  LandingSite_CaptureSiteCategory_2  Species_5   \n",
      "3           Net  Fisher_1798  LandingSite_CaptureSiteCategory_2  Species_6   \n",
      "4       Beached  Fisher_1918  LandingSite_CaptureSiteCategory_2  Species_5   \n",
      "\n",
      "            Tag_1  ... Lost_Tags T_Number CCL_cm  CCW_cm  Weight_Kg      Sex  \\\n",
      "0         CC00147  ...       NaN      NaN  64.70   62.60        NaN  Unknown   \n",
      "1            W442  ...       NaN      NaN  35.85   31.35        NaN  Unknown   \n",
      "2          KE0376  ...       NaN      NaN  51.80   49.20        NaN  Unknown   \n",
      "3         CC00302  ...       NaN      NaN  60.50   59.00        NaN  Unknown   \n",
      "4  NotTagged_0113  ...       NaN      NaN  34.70   33.00        NaN  Unknown   \n",
      "\n",
      "                               TurtleCharacteristics    Status  \\\n",
      "0                             algae at rear of shell  Released   \n",
      "1  multiple b's on front flippers&  a lot of alga...  Released   \n",
      "2                                              clean  Released   \n",
      "3  1 b 3 CS+ calcerous algae at rear end of shell...  Released   \n",
      "4  very lively+ right eye is hanging out + swolle...  Released   \n",
      "\n",
      "      ReleaseSite Date_TimeRelease  \n",
      "0  ReleaseSite_50         22/12/00  \n",
      "1  ReleaseSite_62         28/10/01  \n",
      "2  ReleaseSite_50         01/11/01  \n",
      "3  ReleaseSite_50         11/03/02  \n",
      "4  ReleaseSite_62         08/08/02  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "             CCL_cm        CCW_cm     Weight_Kg\n",
      "count  18038.000000  18035.000000  12653.000000\n",
      "mean      43.090390     40.253904      9.850731\n",
      "std       11.004251      9.933058      9.737378\n",
      "min        2.000000      2.000000      0.020000\n",
      "25%       36.330000     34.000000      5.000000\n",
      "50%       41.300000     39.300000      7.500000\n",
      "75%       47.000000     44.100000     10.800000\n",
      "max      122.750000    106.000000    140.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18062 entries, 0 to 18061\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Rescue_ID              18062 non-null  object \n",
      " 1   Date_TimeCaught        18062 non-null  object \n",
      " 2   Researcher             18062 non-null  object \n",
      " 3   CaptureSite            18062 non-null  object \n",
      " 4   ForagingGround         18062 non-null  object \n",
      " 5   CaptureMethod          18062 non-null  object \n",
      " 6   Fisher                 18062 non-null  object \n",
      " 7   LandingSite            18062 non-null  object \n",
      " 8   Species                18062 non-null  object \n",
      " 9   Tag_1                  18062 non-null  object \n",
      " 10  Tag_2                  18062 non-null  object \n",
      " 11  Lost_Tags              925 non-null    object \n",
      " 12  T_Number               38 non-null     object \n",
      " 13  CCL_cm                 18038 non-null  float64\n",
      " 14  CCW_cm                 18035 non-null  float64\n",
      " 15  Weight_Kg              12653 non-null  float64\n",
      " 16  Sex                    13732 non-null  object \n",
      " 17  TurtleCharacteristics  18021 non-null  object \n",
      " 18  Status                 14429 non-null  object \n",
      " 19  ReleaseSite            17987 non-null  object \n",
      " 20  Date_TimeRelease       11954 non-null  object \n",
      "dtypes: float64(3), object(18)\n",
      "memory usage: 2.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Exploring the data\n",
    "print(data.head())\n",
    "print(data.describe())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e4a24",
   "metadata": {},
   "source": [
    "# Step 3: Data Preprocessing and Feature Engineering\n",
    "In this step, we preprocess the data to prepare it for machine learning modeling. We first drop the Date column since it is not relevant to the prediction task. We then encode the categorical variables using one-hot encoding to convert them into numerical features. Finally, we split the data into training and testing sets using the train_test_split() method from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28387b82",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Captured_Number'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCaptureSite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCaptureMethod\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpecies\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Splitting the data into training and testing sets\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCaptured_Number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCaptured_Number\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5262\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5264\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Captured_Number'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Dropping irrelevant columns\n",
    "data = data.drop(['Date_TimeCaught'], axis=1)\n",
    "\n",
    "# Encoding categorical variables\n",
    "data = pd.get_dummies(data, columns=['CaptureSite', 'CaptureMethod', 'Status', 'Species'])\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X = data.drop(['Captured_Number'], axis=1)\n",
    "y = data['Captured_Number']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447377b",
   "metadata": {},
   "source": [
    "# Step 4: Model Training and Evaluation\n",
    "In this step, we train a machine learning model on the preprocessed data and evaluate its performance using the root mean squared error (RMSE) metric. We use the linear regression algorithm as our model since it is a simple yet effective algorithm for regression tasks. We then use the predict() method to make predictions on the test set and calculate the RMSE using the mean_squared_error() and sqrt() functions from NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e84b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Model Selection\n",
    "#In this step, we will select the appropriate machine learning algorithm(s) to build the model and evaluate its performance. We will perform the following tasks:\n",
    "\n",
    "#Split the data into training and testing sets\n",
    "#Train different machine learning algorithms such as Linear Regression, Random Forest Regression, and XGBoost Regression on the training data\n",
    "#Evaluate the performance of the models using the root mean squared error (RMSE) and R-squared (R2) values on the testing data\n",
    "#Select the best-performing model(s) based on the evaluation metrics\n",
    "\n",
    "import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "read in the pre-processed data\n",
    "df = pd.read_csv('preprocessed_data.csv')\n",
    "\n",
    "split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('Captured_Number', axis=1),\n",
    "df['Captured_Number'],\n",
    "test_size=0.2,\n",
    "random_state=42)\n",
    "\n",
    "train a linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "train a random forest regression model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "train an XGBoost regression model\n",
    "xgbr = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgbr.fit(X_train, y_train)\n",
    "\n",
    "evaluate the performance of the models on the test set\n",
    "lr_rmse = mean_squared_error(y_test, lr.predict(X_test), squared=False)\n",
    "lr_r2 = r2_score(y_test, lr.predict(X_test))\n",
    "\n",
    "rf_rmse = mean_squared_error(y_test, rf.predict(X_test), squared=False)\n",
    "rf_r2 = r2_score(y_test, rf.predict(X_test))\n",
    "\n",
    "xgbr_rmse = mean_squared_error(y_test, xgbr.predict(X_test), squared=False)\n",
    "xgbr_r2 = r2_score(y_test, xgbr.predict(X_test))\n",
    "\n",
    "print(f'Linear Regression RMSE: {lr_rmse}, R2: {lr_r2}')\n",
    "print(f'Random Forest RMSE: {rf_rmse}, R2: {rf_r2}')\n",
    "print(f'XGBoost RMSE: {xgbr_rmse}, R2: {xgbr_r2}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05195bc",
   "metadata": {},
   "source": [
    "# Step 5: Making Predictions and Creating Submission File\n",
    "In this step, we use the trained model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Model Optimization\n",
    "#In this step, we will optimize the parameters of the selected model(s) to improve their performance. We will perform the following tasks:\n",
    "\n",
    "#Tune the hyperparameters of the selected models using techniques such as grid search and random search\n",
    "#Evaluate the performance of the optimized models on the test set\n",
    "#Select the best-performing model(s) based on the evaluation metrics\n",
    "\n",
    "import necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "tune the hyperparameters of the random forest model using grid search\n",
    "param_grid = {\n",
    "'n_estimators': [100, 200, 300],\n",
    "'max_depth': [10, 20, 30],\n",
    "'min_samples_split': [2, 4, 6],\n",
    "'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestRegressor(random_state=42),\n",
    "param_grid=param_grid,\n",
    "cv=5,\n",
    "n_jobs=-1,\n",
    "verbose=2)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the new data\n",
    "new_data = pd.read_csv('new_data.csv')\n",
    "new_data = pd.get_dummies(new_data, columns=['CaptureSite', 'RescueMethod', 'Status', 'TagType'])\n",
    "new_X = new_data.drop(['Captured_Number'], axis=1)\n",
    "new_y_pred = model.predict(new_X)\n",
    "\n",
    "# Creating submission file\n",
    "submission = pd.DataFrame({'ID': new_data['ID'], 'Captured_Number': new_y_pred})\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
